name: llama-cpp-python

on: workflow_dispatch

jobs:
  build_wheels_linux:
    name: Build wheels on ubuntu-latest
    runs-on: ubuntu-latest
    container: quay.io/pypa/manylinux_2_28_x86_64
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]

    defaults:
      run:
        shell: bash -el {0}

    steps:
      - uses: actions/checkout@v3
        with:
          repository: abetlen/llama-cpp-python
          ref: v0.2.21
          submodules: true

      - uses: conda-incubator/setup-miniconda@v2
        with:
          python-version: ${{ matrix.python-version }}
          channels: conda-forge
          miniforge-version: latest

      - name: echo conda-prefix
        run: |
          echo "LD_LIBRARY_PATH=$CONDA_PREFIX/lib" >> "$GITHUB_ENV"
          echo $CONDA_PREFIX/bin >> "$GITHUB_PATH"

      - run: conda install -y -c "nvidia/label/cuda-12.1.1" cuda
      - run: conda install -y build auditwheel

      - name: build wheel
        run: python -m build --wheel .
        env:
          CMAKE_ARGS: "-DLLAMA_CUBLAS=on"
          FORCE_CMAKE: "1"

      - run: auditwheel show dist/*

      - run: auditwheel repair --plat manylinux_2_28_x86_64 --exclude libllama.so --exclude libllava.so --exclude libggml_shared.so dist/*

      - uses: actions/upload-artifact@v3
        with:
          path: ./wheelhouse/*.whl

  test_wheel:
    name: Test wheels
    needs: [build_wheels_linux]
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest]

    steps:
      - uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - uses: actions/download-artifact@v3
        with:
          name: artifact
          path: dist

      - run: pip install -f dist llama-cpp-python

      - run: python -c "import llama_cpp"
